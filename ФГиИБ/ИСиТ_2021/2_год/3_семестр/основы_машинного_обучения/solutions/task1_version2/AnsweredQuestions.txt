1) Что такое логическая регрессия? Какие основные плюсы?
Логистическая регрессия - это модель бинарной классификации, использующая функцию сигмоиды для расчета вероятности на основе коэффициентов функции независимой переменной, рассчитанных в процессе обучения. Логистическая регрессия хорошо подходит для решения задач двоичной классификации — мы просто назначаем разным категориям значения 0 и 1 соответственно. Еще плюсом является возможность интерпретации результатов и понимания влияния каждой переменной на исход модели.

(для 2 и 3 вопросов) Деревья решений — это модели МО, используемые для прогнозирования через циклический просмотр каждой функции в наборе данных один за одним. 
2) Что такое (решающее) дерево? Какие основные плюсы?
Решающее дерево - модель машинного обучения, основанная на разделении выборок объектов по их критериям до тех пор, пока не будет установлен класс объекта, или же не будет получено значение (для задач регрессии). Основным плюсом является простота интерпретации и понимание логики принятия решений. Но построение дерева бывает сложными, потому что решение на какие разветвления (характеристики) делить ваши данные — это математически сложная задача. Для разрешения этой задачи, используют алгоритм случайного леса. Плюсы этого метода по сравнению со случайным лесом в том, что он проще и занимает меньше места и вычислительной мощности.

3) Что такое случайный лес? Какие основные плюсы?
Случайный лес - алгоритм, использующий совокупность множества отдельных решающих деревьев. Алгоритм является универсальным, так как он может использоваться для классификации, регрессии и т.д. Случайный лес использует бэггинг и случайность признаков при построении каждого отдельного дерева, чтобы создать некоррелированный лес из деревьев, прогноз которого более точен, чем прогноз любого отдельного дерева. Решение алгоритм принимает либо по усредненным ответам всех деревьев (для задач классификации), либо по "голосованию" (для задач регрессии).
Преимущества – гибкость, довольно высокая точность (даже без передачи гиперпараметров), а также распараллеливание вычислений. Декорреляция для уменьшения дисперсии — главное преимущество в использовании случайных лесов в сравнении с деревьями решений, построенными вручную.
Недостаток – большой размер моделей и, следовательно, большие затраты на вычисления.

4) Чем беггинг отличается от бустинга?
Bagging (беггинг) и Boosting (бустинг) - это два подхода к созданию ансамбля моделей машинного обучения (несколько моделей обучаются и их результаты объединяются для конечного ответа). Основное отличие между ними заключается в том, что Bagging использует одинаковые модели и обучается параллельно, а Boosting - разные модели, которые взаимодействуют между собой последовательно, обучаясь на ошибках друг друга.

5) Почему дерево для задачи регрессии не следует использовать? Какой основной минус?
Дерево для задачи регрессии не следует использовать, потому что оно может быть переобученным и не давать точных предсказаний. Основной минус дерева для задач регрессии заключается в том, что оно часто стремится максимизировать точность на обучающих данных за счет увеличения глубины дерева. В результате, дерево может быть слишком сложным и переобучаться на обучающих данных, что приводит к низкой обобщающей способности на новых данных (на новых данных, модель может уже не работать или плохо работать из-за переобучения на тестовых данных).

6) Какие из алгоритмов интерпретируемые?
Логистическая регрессия и решающее дерево являются интерпретируемыми алгоритмами, они позволяют понять влияние каждой переменной на исход модели и объяснить принятые решения. 
Логистическая регрессия используется для бинарной классификации, поэтому она может быть применена для предсказания, вероятности сердечного приступа у пациентов. Она позволяет понимать, какие факторы оказывают наибольшее влияние на прогноз, и оценивать вес каждого признака в модели.
Решающие деревья также могут быть использованы для классификации и анализа данных пациентов для нашей задачи. Они позволяют пошагово проанализировать, какое условие должно быть выполнено, чтобы определить вероятность сердечного приступа. В этом случае, каждый узел дерева может рассматриваться как признак, который оказывает влияние на прогноз.

7) Какая метрика важна в нашей задаче? Accuracy, precision, recall, roc-auc?
В нашей задаче, больше всего важна метрика recall, так как нам нужно правильно идентифицировать наибольшее число пациентов с высоким риском сердечного приступа, важно чтобы модель не пропускала реально больных людей, поэтому recall должен быть максимальным. Precision является второй по важности метрикой, которая покажет нам долю правильных ответов среди всех предсказаний. Roc-auc также важная характеристика, однако не такая важная в нашей задаче как две предыдущие, она покажет нам насколько хорошо модель разделяет классы, а именно какую площадь занимает кривая на графике. Чем ближе значение ROC-AUC к 1, тем лучше модель разделяет классы и тем выше качество модели. Если значение ROC-AUC меньше 0.5, то модель строит прогнозы хуже, чем случайные предсказания. Однако для нас важнее recall и precision так как они отражают способность нашей модели находить реально больных людей. Accuracy не является важной метрикой в нашей задаче так как она бесполезна в задачах с неравными классами, количество пациентов без риска приступа обычно больше количества пациентов с риском. Однако в нашем случае на него можно ссылаться, так как выборка сбалансирована, но лучше брать метрики выше, они точнее.

Ниже представлены объяснения метрик с помощью матрицы ошибок (confusion matrix) и своими словами (в кавычках""):
Метрика Accuracy показывает долю правильных ответов алгоритма, является почти неиспользуемой, так как бесполезна в задачах с неравными классами, потому что эта метрика общая для всех классов, для нашей задачи важны метрики с отдельными показателями качества классов. Accuracy = TP + TN / TP + TN + FP + FN
"Нужным исходом в нашей задаче является предсказание того, что у человека есть высокий риск болезни, а не нужным считается низкий риск болезни"
Метрика Precision (точность) показывает, насколько алгоритм верно совершает предсказания. "Является соотношением верно предсказанных нужных исходов и всех предсказанных нужных исходов (суммой верно предсказанных нужных исходов и неверно предсказанных нужных исходов)." Precision = TP / TP + FP 
Метрика Recall (полнота) показывает, насколько алгоритм вообще может предсказывать нужный исход. "Является соотношением верно предсказанных нужных исходов и вообще всех нужных исходов (суммой верно предсказанных и не предсказанных, но верных исходов)." Recall = TP / TP + FN
Метрика ROC-AUC (площадь под кривой ошибок) является одним из способов оценить модель в целом, не привязываясь к конкретному порогу. Кривая представляет из себя линию от 0:0 до 1:1 в координатах TPR - Recall и FPR - (инверсия Recall) "соотношение всех не верно предсказанных не нужных исходов и всех не нужных исходов (суммой не нужных не верно предсказанных и всех не предсказанных не нужных исходов)." TPR = TP / TP + FN , FPR = FP / FP + TN